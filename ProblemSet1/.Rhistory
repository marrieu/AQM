model3 <-lm( sqrt(Sales)~TV+Radio+Newspaper,data=adv.scaled)
model <-lm(sqrt(Sales)~TV+Radio+Newspaper,data=adv2)
summary(model)
plot(model)
model <-lm(sqrt(Sales)~TV*Radio+Newspaper,data=adv2)
summary(model)
plot(model)
model <-lm(sqrt(Sales)~TV+Radio:TV,data=adv2)
summary(model)
plot(model)
model2 <-lm(Sales~I(TV^2)+TV+Radio:TV,data=adv2)
summary(model2)
plot(model)
model <-lm(sqrt(Sales)~TV+Radio:TV,data=adv2)
plot(model)
model <-lm(Sales~TV+Radio:TV,data=adv2)
bc<-boxcox(model)
bc$x[which.max(bc$y)]
model <-lm(I(Sales^1.7)~TV+Radio:TV,data=adv2)
summary(model)
plot(model)
model <-lm(I(Sales^2)~TV+Radio:TV,data=adv2)
plot(model)
model <-lm(I(Sales^1.7)~TV+Radio:TV,data=adv2)
plot(model)
model$residuals
model <-lm(Sales~TV+Radio+Newspaper,data=adv)
adv$error = model$residuals
View(adv)
plot(model)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(error^2))
plot(model4)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(abs(error))
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(abs(error)))
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(abs(error)))
plot(model4)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(sqr(abs(error))))
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(sqrt(abs(error))))
plot(model4)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/error)
model$residuals
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/abs(error))
plot(model4)
plot(model)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/error^2)
plot(model4)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(error^2))
plot(model4)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(error^2))
plot(model4)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/abs(error))
plot(model4)
summary(model4)
plot(model)
summary(model)
adv$error = abs(model$residuals)
model5 <-lm(Sales~error,data=adv)
summar(model5)
summary(model5)
plot(model5)
model5$fitted.values
adv$sig = abs(model5$fitted.values)
adv$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv, weights = 1/(sig)^2)
summary(model4)
plot(model4)
plot(model)
adv2 = adv[-131,]
model <-lm(Sales~TV+Radio+Newspaper,data=adv2)
adv$error = abs(model$residuals)
adv2$error = abs(model$residuals)
model5 <-lm(Sales~error,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/(sig)^2)
summary(model4)
plot(model4)
summary(model4)
sumamry(model)
summary(model)
plot(model)
adv2$error = model$residuals^2
model5 <-lm(Sales~error,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/(sig)^2)
summary(model4)
plot(model4)
model5 <-lm(model$fitted.values~model$residuals^2,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/(sig)^2)
summary(model4)
plot(model4)
plot(model4)
adv2$error = model$residuals^2
model5 <-lm(Sales~error,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/sig)
plot(model4)
model <-lm(Sales~TV+Radio+Newspaper,data=adv)
plot(model)
summary(model)
model <-lm(sqrt(Sales)~TV+Radio+Newspaper,data=adv)
summary(model)
plot(model)
model <-lm(sqrt(Sales)~TV+Radio+Newspaper,data=adv2)
summary(model)
par(mfrow=c(2,2))
plot(model)
model <-lm(Sales~TV+Radio+Newspaper,data=adv2)
summary(model)
summary(model)
plot(model)
par(mfrow=c(1,1))
model <-lm(sqrt(Sales)~TV+Radio+Newspaper,data=adv2)
plot(model)
model <-lm(I(Sales^1.7)~TV+Radio:TV,data=adv2)
plot(model)
plot(model)
model <-lm(Sales^1.7~TV+Radio:TV,data=adv2)
plot(model)
model <-lm(Sales~TV+Radio+Newspaper,data=adv2)
plot(model)
par(mfrow=c(2,2))
adv2$error = model$residuals^2
model5 <-lm(Sales~error,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/sig)
summary(model4)
plot(model4)
#  feature scaling
adv.scaled <-as.data.frame(scale(adv2))
model3 <-lm(Sales~TV+Radio+Newspaper,data=adv.scaled)
summary(model3)
plot(model3)
model3 <-lm(Sales~I(TV^2)+TV+Radio:TV,data=adv.scaled)
summary(model3)
model2 <-lm(Sales~I(TV^2)+TV+Radio:TV,data=adv2)
summary(model2)
model3 <-lm(Sales~TV+Radio+Newspaper,data=adv.scaled)
summary(model3)
plot(model3)
model <-lm(Sales~TV+Radio+Newspaper,data=adv2)
summary(model)
plot(model)
model2 <-lm(sqrt(Sales)~TV+Radio+Newspaper,data=adv2)
summary(model2)
plot(model2)
model3 <-lm(Sales~I(TV^2)+TV+Radio:TV,data=adv2)
summary(model3)
plot(model3)
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv.scaled)
summary(model4)
plot(model4)
model5 <-lm(Sales~I(TV^2)+TV+Radio:TV,data=adv.scaled)
summary(model5)
plot(model5)
model5 <-lm(Sales~I(TV^2)+Radio:TV,data=adv.scaled)
summary(model5)
model5 <-lm(Sales~I(TV^2)+TV+Radio+Radio:TV,data=adv.scaled)
summary(model5)
plot(model5)
model5 <-lm(Sales~TV+Radio+Radio:TV,data=adv.scaled)
summary(model5)
summary(model5)
plot(model5)
model5 <-lm(Sales~TV+Radio+Radio:TV+Newspaper,data=adv.scaled)
summary(model5)
plot(model5)
model5 <-lm(Sales~TV+Radio+Radio:TV,data=adv.scaled)
par(mfrow=c(1,1))
plot(model5)
par(mfrow=c(2,2))
plot(model5)
model5 <-lm(Sales~I(TV^2)+Radio+Radio:TV,data=adv.scaled)
summary(model5)
model5 <-lm(log(Sales)~TV+Radio+Radio:TV,data=adv.scaled)
model5 <-lm(sqrt(Sales)~TV+Radio+Radio:TV,data=adv.scaled)
model5 <-lm(Sales~TV+Radio+Radio:TV,data=adv.scaled)
model6 <-lm(Sales~TV+Radio+Radio:TV,data=adv2)
summary(model6)
plot(model6)
model6 <-lm(sqrt(Sales)~TV+Radio+Radio:TV,data=adv2)
summary(model6)
plot(model6)
model6 <-lm(Sales~I(TV^2)+Radio+Radio:TV,data=adv2)
summary(model6)
plot(model6)
model6 <-lm(Sales~I(TV^1.5)+Radio+Radio:TV,data=adv2)
summary(model6)
plot(model6)
model6 <-lm(Sales~TV+Radio+I(Radio:TV^2),data=adv2)
model6 <-lm(Sales~TV+Radio+I((Radio:TV)^2),data=adv2)
model6 <-lm(Sales~TV+Radio+I(Radio^2):I(TV^2),data=adv2)
summary(model6)
plot(model6)
model6 <-lm(log(Sales)~TV+Radio+I(Radio^2):I(TV^2),data=adv2)
summary(model6)
plot(model6)
model6 <-lm(log(Sales)~TV+Radio+Radio:TV,data=adv2)
summary(model6)
plot(model6)
summary(model)
plot(model)
model2 <-lm(log(Sales)~TV+Radio+Newspaper,data=adv2)
summary(model2)
plot(model2)
model3 <-lm(Sales~I(TV^2)+TV+Radio:TV,data=adv2)
summary(model3)
plot(model3)
model6 <-lm(Sales~TV+Radio+Radio:TV,data=adv2)
summary(model6)
plot(model6)
summary(model3)
summary(model6)
summary(model3)
summary(model6)
View(adv)
View(adv)
summary(model)
adv2$fit = model$fitted.values
model5 <-lm(error~fit,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/sig)
summary(model4)
plot(model4)
model5 <-lm(error~sales,data=adv2)
model5 <-lm(error~Sales,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/sig)
model5 <-lm(error~fit,data=adv2)
adv2$sig = model5$fitted.values
model4 <-lm(Sales~TV+Radio+Newspaper,data=adv2, weights = 1/sig)
summary(model4)
plot(model4)
library(tidyverse)
# get data
adv <- read_csv('Advertising.csv')
View(adv)
# add column of 1's
adv$x0 <- 1
# create the x- matrix of explanatory variables
x <- as.matrix(adv[c('x0','TV')])
# create the y-matrix of dependent variables
y <- as.matrix(adv['Sales'])
m <- nrow(y)
x.scaled <- x
for (i in 2:dim(x)[2]) {
x.scaled[,i] <- (x[,i] - mean(x[,i]))/sd(x[,i])
}
x
theta <- matrix(rnorm(dim(x)[2]), nrow=1)
theta
pi
x %*% t(theta)
(x %*% t(theta))^2
sum((x %*% t(theta))^2)
LLi <- function(theta,sigma,x,y) {
L=-(1/m)*log(2*pi)-m*log(sigma)-1/(2*sigma^2)*sum((x %*% t(theta))^2)
retun(L)
}
sigma = 2
test=LLi(theta,sigma,x,y)
LLi <- function(theta,sigma,x,y) {
L=-(1/m)*log(2*pi)-m*log(sigma)-1/(2*sigma^2)*sum((x %*% t(theta))^2)
return(L)
}
test=LLi(theta,sigma,x,y)
test
LLi <- function(theta,sigma) {
L=-(1/m)*log(2*pi)-m*log(sigma)-1/(2*sigma^2)*sum((x %*% t(theta))^2)
return(L)
}
test=LLi(theta,sigma)
test
test=LLi(theta,sigma)
test
sigma^-1
(y - (x %*% t(theta)) )
(t(x) %*% (y - (x %*% t(theta)) ))
sum(y - (x %*% t(theta)) )
(m*sigma^-1)-(sigma^-3)*sum(y - (x %*% t(theta)) )
-(m*sigma^-1)+(sigma^-3)*sum(y - (x %*% t(theta)) )
(1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
grad <- function(x, y, theta,sigma) {
gradient <- (1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
sig <- -(m*sigma^-1)+(sigma^-3)*sum(y - (x %*% t(theta)) )
return(t(gradient),sig)
}
grad(x, y, theta,sigma)
grad <- function(x, y, theta,sigma) {
gradient <- (1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
sig <- -(m*sigma^-1)+(sigma^-3)*sum(y - (x %*% t(theta)) )
return(t(gradient))
}
grad(x, y, theta,sigma)
sig(x, y, theta,sigma)
sig <- function(x, y, theta,sigma) {
sig <- -(m*sigma^-1)+(sigma^-3)*sum(y - (x %*% t(theta)) )
return(sig)
}
sig(x, y, theta,sigma)
rnorm(1)
# define the gradient function dJ/dtheata
grad <- function(x, y, theta,sigma) {
gradient <- (1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
return(t(gradient))
}
sig <- function(x, y, theta,sigma) {
sig <- -(m*sigma^-1)+(sigma^-3)*sum(y - (x %*% t(theta)) )
return(sig)
}
# define gradient descent  algorithm
grad.descent <- function(x, num_iter, alpha = .01){
theta <- matrix(rnorm(dim(x)[2]), nrow=1) # random initialize the parameters
sigma <- rnorm(1)
for (i in 1:num_iter) {
theta <- theta - alpha*grad(x, y, theta, sigma)
sigma <- sigma - alpha*sig(x, y, theta, sigma)
}
return(theta)
}
set.seed(3)
print(grad.descent(x,100000,alpha = .00001))
print(grad.descent(x.scaled,1000, alpha = .01))
# define the gradient function dJ/dtheata
grad <- function(x, y, theta,sigma) {
gradient <- -(1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
return(t(gradient))
}
sig <- function(x, y, theta,sigma) {
sig <- (m*sigma^-1)-(sigma^-3)*sum(y - (x %*% t(theta)) )
return(sig)
}
grad.descent <- function(x, num_iter, alpha = .01){
theta <- matrix(rnorm(dim(x)[2]), nrow=1) # random initialize the parameters
sigma <- rnorm(1)
for (i in 1:num_iter) {
theta <- theta - alpha*grad(x, y, theta, sigma)
sigma <- sigma - alpha*sig(x, y, theta, sigma)
}
return(theta)
}
set.seed(3)
# results with feature normalizating
print(grad.descent(x.scaled,1000, alpha = .01))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .01))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .01))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .01))
set.seed(3)
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .01))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .0001))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .0001))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .0001))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .0001))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .0001))
# results using lm function without feature normalizating
summary(lm(y ~ x[, 2:4]))
# create the x- matrix of explanatory variables
x <- as.matrix(adv[c('x0','TV')])
# results using lm function without feature normalizating
summary(lm(y ~ x))
# results using lm function with feature normalizating
summary(lm(y ~ x.scaled))
x
x.scaled
y
# results using lm function with feature normalizating
summary(lm(y ~ x.scaled))
# results using lm function with feature normalizating
summary(lm(y ~ x.scaled[,2]))
# results using lm function without feature normalizating
summary(lm(y ~ x[,2]))
# results using lm function without feature normalizating
summary(lm(y ~ x[,2]))
# results with feature normalizating
print(grad.descent(x.scaled,10000, alpha = .0001))
# results with feature normalizating
print(grad.descent(x.scaled,100000, alpha = .0001))
library(tidyverse)
# get data
adv <- read_csv('Advertising.csv')
# add column of 1's
adv$x0 <- 1
# create the x- matrix of explanatory variables
x <- as.matrix(adv[c('x0','TV')])
# create the y-matrix of dependent variables
y <- as.matrix(adv['Sales'])
m <- nrow(y)
x.scaled <- x
for (i in 2:dim(x)[2]) {
x.scaled[,i] <- (x[,i] - mean(x[,i]))/sd(x[,i])
}
# define the gradient functions for theta0 and theta1
grad <- function(x, y, theta,sigma) {
gradient <- -(1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
return(t(gradient))
}
# define the gradient functions for sigma
sig <- function(x, y, theta,sigma) {
sig <- (m*sigma^-1)-(sigma^-3)*sum(y - (x %*% t(theta)) )
return(sig)
}
grad <- function(x, y, theta,sigma) {
gradient <- -(1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
return(t(gradient))
}
grad <- function(x, y, theta,sigma) {
gradient <- (1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
return(t(gradient))
}
# define the gradient functions for sigma
sig <- function(x, y, theta,sigma) {
sig <- (m*sigma^-1)-(sigma^-3)*sum(y - (x %*% t(theta)) )
return(sig)
}
grad.descent <- function(x, num_iter, alpha = .01){
theta <- matrix(rnorm(dim(x)[2]), nrow=1) # random initialize the parameters
sigma <- rnorm(1)
for (i in 1:num_iter) {
theta <- theta - alpha*grad(x, y, theta, sigma)
sigma <- sigma - alpha*sig(x, y, theta, sigma)
}
return(theta)
}
set.seed(3)
# Estimation by Maximum Likelihood using Gradint Decent
# results with feature normalizating
print(grad.descent(x.scaled,100000, alpha = .0001))
# define the gradient functions for theta0 and theta1
grad <- function(x, y, theta,sigma) {
gradient <- -(1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
return(t(gradient))
}
# define the gradient functions for sigma
sig <- function(x, y, theta,sigma) {
sig <- (m*sigma^-1)-(sigma^-3)*sum(y - (x %*% t(theta)) )
return(sig)
}
grad.descent <- function(x, num_iter, alpha = .01){
theta <- matrix(rnorm(dim(x)[2]), nrow=1) # random initialize the parameters
sigma <- rnorm(1)
for (i in 1:num_iter) {
theta <- theta - alpha*grad(x, y, theta, sigma)
sigma <- sigma - alpha*sig(x, y, theta, sigma)
}
return(theta)
}
set.seed(3)
# Estimation by Maximum Likelihood using Gradint Decent
# results with feature normalizating
print(grad.descent(x.scaled,100000, alpha = .0001))
# results using lm function with feature normalizating (least-squares )
summary(lm(y ~ x.scaled[,2]))
library(tidyverse)
# get data
adv <- read_csv('Advertising.csv')
# add column of 1's
adv$x0 <- 1
# create the x- matrix of explanatory variables
x <- as.matrix(adv[c('x0','TV')])
# create the y-matrix of dependent variables
y <- as.matrix(adv['Sales'])
m <- nrow(y)
#  feature normalization
x.scaled <- x
for (i in 2:dim(x)[2]) {
x.scaled[,i] <- (x[,i] - mean(x[,i]))/sd(x[,i])
}
# define the gradient functions for theta0 and theta1
grad <- function(x, y, theta,sigma) {
gradient <- -(1/sigma^2)* (t(x) %*% (y - (x %*% t(theta)) ))
return(t(gradient))
}
# define the gradient functions for sigma
sig <- function(x, y, theta,sigma) {
sig <- (m*sigma^-1)-(sigma^-3)*sum(y - (x %*% t(theta)) )
return(sig)
}
# define gradient descent  algorithm
grad.descent <- function(x, num_iter, alpha = .01){
theta <- matrix(rnorm(dim(x)[2]), nrow=1) # random initialize the parameters
sigma <- rnorm(1) # random initialize sigma
for (i in 1:num_iter) {
theta <- theta - alpha*grad(x, y, theta, sigma)
sigma <- sigma - alpha*sig(x, y, theta, sigma)
}
return(theta)
}
set.seed(3)
# Estimation by Maximum Likelihood using Gradint Decent
# results with feature normalizating
print(grad.descent(x.scaled,100000, alpha = .0001))
#         x0       TV
#Sales 14.02964 4.082675
summary(lm(y ~ x.scaled[,2]))
test<-lm(y ~ x.scaled[,2])
test$coefficients
# results using lm function with feature normalizating (least-squares )
summary(lm(y ~ x.scaled[,2])$coefficients)
# results using lm function with feature normalizating (least-squares )
summary(lm(y ~ x.scaled[,2]))
# results using lm function with feature normalizating (least-squares )
model<-lm(y ~ x.scaled[,2])
print(model$coefficients)
